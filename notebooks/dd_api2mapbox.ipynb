{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import geojson\n",
    "import requests\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_url = 'http://pl-tc012.xtr.deltares.nl:8080/FewsWebServices/rest/digitaledelta/2.0/locations'\n",
    "nodes_url = 'http://pl-tc012.xtr.deltares.nl:8080/FewsWebServices/rest/digitaledelta/2.0/nodes'\n",
    "observation_types_url = 'http://pl-tc012.xtr.deltares.nl:8080/FewsWebServices/rest/digitaledelta/2.0/observationTypes'\n",
    "timeseries_url = 'http://pl-tc012.xtr.deltares.nl:8080/FewsWebServices/rest/digitaledelta/2.0/timeseries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first request and inspect it to get the paging information\n",
    "\n",
    "def paging_iter(url):\n",
    "    \"\"\"iterate over all pages in a url\"\"\"\n",
    "    resp = requests.get(url).json()\n",
    "    page_size = resp['paging']['maxPageSize']\n",
    "    count = resp['paging']['totalObjectCount']\n",
    "    # Now loop over all pages\n",
    "    n = math.ceil(count / page_size)\n",
    "\n",
    "    # store each json result\n",
    "    # progress please\n",
    "    for i in tqdm.tqdm_notebook(range(n)):\n",
    "        # fire the request. It's not faster in parallel (tested with asyncio), so I keep it serial.\n",
    "        resp = requests.get(url, dict(page=i + 1, pageSize=page_size))\n",
    "        yield resp.json()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafd5b48a297475b8d21fe1c2e9d404c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=175), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "locations_resps = list(paging_iter(locations_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319e2b270c434bc2a58b795b8c0d80f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1665), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# collect information about the information per location\n",
    "# one request per location\n",
    "timeseries_resps = list(paging_iter(timeseries_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all the features\n",
    "features = []\n",
    "for resp in locations_resps:\n",
    "    results = resp['results']\n",
    "    for feature in results:\n",
    "        feature['id'] = feature['properties']['locationId']\n",
    "        features.append(feature)\n",
    "        \n",
    "features_by_id = {\n",
    "    feature['id']: feature\n",
    "    for feature \n",
    "    in features\n",
    "}\n",
    "\n",
    "timeseries = []\n",
    "for resp in timeseries_resps:\n",
    "    timeseries.extend(resp['results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "for series in timeseries:\n",
    "    id = series['location']['properties']['locationId']\n",
    "    feature = features_by_id[id]\n",
    "    quantities = feature['properties'].get('quantities', [])\n",
    "    quantity = series['observationType']['quantity']\n",
    "    if quantity not in quantities:\n",
    "        quantities.append(quantity)\n",
    "    feature['properties']['quantities'] = quantities\n",
    "    features_by_id[feature['id']] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the features\n",
    "collection = geojson.FeatureCollection(features=list(features_by_id.values()))\n",
    "with open('pl-tc012.geojson', 'w') as f:\n",
    "    geojson.dump(collection, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lUploading data source  [####################################]  100%             \u001b[?25h\n",
      "{\"id\":\"cjxpw7j9n0lt22omw307oot3x\",\"name\":\"pl-tc012\",\"complete\":false,\"error\":null,\"created\":\"2019-07-05T09:24:48.599Z\",\"modified\":\"2019-07-05T09:24:48.599Z\",\"tileset\":\"siggyf.pl-tc012\",\"owner\":\"siggyf\",\"progress\":0}\n"
     ]
    }
   ],
   "source": [
    "# Create tiles until level 12 or something.... with tippecanoe\n",
    "\n",
    "# let's upload it\n",
    "!mapbox upload siggyf.pl-tc012 pl-tc012.geojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
